import os
import asyncio
import logging
import json
import time
import cv2
import numpy as np
import time
import redis
import base64
import logging.config
from contextlib import asynccontextmanager
from openai import OpenAI
from fastapi import FastAPI, WebSocket, WebSocketDisconnect
from fastapi.middleware.cors import CORSMiddleware
import uvicorn
from api.models import Sesion, Paciente, MetricaPostural, PosturaCount
from api.database import Base, engine, SessionLocal
from posture_monitor import PostureMonitor
from api.routers import sesiones, pacientes, metricas, analysis, postura_counts, timeline, calibracion
from datetime import datetime

r = redis.Redis(host="redis", port=6379, decode_responses=True)


LOGGING_CONFIG = {
    "version": 1,
    "disable_existing_loggers": False,
    "formatters": {
        "default": {
            "format": "%(asctime)s [%(levelname)s] %(name)s: %(message)s",
            "datefmt": "%Y-%m-%d %H:%M:%S"
        }
    },
    "handlers": {
        "console": {
            "class": "logging.StreamHandler",
            "formatter": "default",
            "level": "DEBUG",
            "stream": "ext://sys.stdout"
        }
    },
    "loggers": {
        "posture_monitor": {
            "handlers": ["console"],
            "level": "DEBUG",
            "propagate": False
        },
        "api_analysis_worker": {
            "handlers": ["console"],
            "level": "DEBUG",
            "propagate": False
        }, 
        "uvicorn.access": {
            "handlers": ["console"],
            "level": "WARNING",
            "propagate": False
        },
        "uvicorn.error": {
            "handlers": ["console"],
            "level": "WARNING",
            "propagate": False
        },

    },
    # El logger ra√≠z queda en WARNING, filtrando todo lo dem√°s
    "root": {
        "handlers": ["console"],
        "level": "WARNING"
    }
}
logging.config.dictConfig(LOGGING_CONFIG)
logger = logging.getLogger("shpd-backend")
logger.setLevel(logging.DEBUG)


# ‚Äî‚Äî‚Äî CONFIGURACI√ìN DEL CLIENTE DE OPENAI ‚Äî‚Äî‚Äî
API_KEY = os.getenv(
    "OPENAI_API_KEY",
    "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"
)
client = OpenAI(api_key=API_KEY)
MODEL = "gpt-4o-mini"

def build_openai_messages(b64: str) -> list[dict]:
    """
    Construye la lista de mensajes para enviar a la API de OpenAI GPT-4 Vision,
    solicitando que, en lugar de booleanos, devuelva porcentajes (0‚Äì100) 
    que indiquen la predominancia estimada de cada postura.

    - b64: cadena Base64 de la imagen (sin prefijo MIME).
    Devuelve una lista de diccionarios con los roles y contenidos formateados.
    """

    # 1) Prompt del sistema: define el rol y las 13 posturas, 
    #    indicando que debe devolver porcentajes 0‚Äì100 en JSON.
    SYSTEM_PROMPT = """Eres un asistente de clasificaci√≥n de posturas basado en visi√≥n.
Cuando recibas una imagen, analiza la postura de la persona 
qu√© tan predominante es cada una de las siguientes 13 posturas al sentarse.
Las posturas son:
- Sentado erguido
- Inclinaci√≥n hacia adelante
- Inclinaci√≥n hacia atr√°s
- Inclinaci√≥n hacia la izquierda
- Inclinaci√≥n hacia la derecha
- Ment√≥n en mano
- Piernas cruzadas
- Rodillas elevadas o muy bajas
- Hombros encogidos
- Brazos sin apoyo
- Cabeza adelantada
- Encorvarse hacia adelante
- Sentarse en el borde del asiento


Determina y devuelve estrictamente un objeto JSON, sin cercos de c√≥digo circundante,
formato markdown o cualquier texto extra-s√≥lo el propio JSON¬ª.
En lugar de un valor booleano, para cada postura devuelve un porcentaje entre 0 y 100 
(0 = nada probable / no se aprecia, 100 = completamente predominante). 
El JSON debe tener cada postura como clave exacta y su valor num√©rico como porcentaje.
Proporciona √öNICAMENTE el objeto JSON como salida, sin texto adicional."""

    # 2) Prompt del usuario (texto): instrucci√≥n breve para clasificar la imagen con porcentajes.
    user_prompt = (
        "Analiza la imagen adjunta y genera un JSON con los 13 tipos de postura "
        "listados en el mensaje de sistema. Para cada postura, coloca un n√∫mero "
        "entero de 0 a 100 que indique la probabilidad o predominancia estimada. "
        "No incluyas explicaciones; solo devuelve el objeto JSON."
    )

    # 3) Construcci√≥n de content para el usuario, usando tipo "text" + "image_url"
    user_content = [
        {"type": "text", "text": user_prompt},
        {
            "type": "image_url",
            "image_url": {
                "url": f"data:image/jpeg;base64,{b64}",
                "details": "high"
            }
        }
    ]

    # 4) Devolver la lista de mensajes con roles "system" y "user"
    return [
        {"role": "system", "content": SYSTEM_PROMPT},
        {"role": "user",   "content": user_content}
    ]

# ‚Äî‚Äî‚Äî COLAS AS√çNCRONAS ‚Äî‚Äî‚Äî
processed_frames_queue: asyncio.Queue = asyncio.Queue(maxsize=10)
api_analysis_queue: asyncio.Queue = asyncio.Queue()
_triggered_sessions = set()  # para disparar clasificaci√≥n s√≥lo una vez por umbral

# ‚Äî‚Äî‚Äî WORKER PARA LLAMADAS A OPENAI ‚Äî‚Äî‚Äî
async def api_analysis_worker():
    """
    Worker que consume de api_analysis_queue payloads con:
      { session_id, b64, exercise }
    Llama a OpenAI y guarda el JSON resultante en Redis bajo analysis:{session_id}.
    """
    loop = asyncio.get_running_loop()
    logger.debug("üîÑ API analysis worker iniciado")
    while True:
        payload = await api_analysis_queue.get()
        session_id = payload["session_id"]
        jpeg        = payload["jpeg"]
        bad_time    = payload["bad_time"]
        b64 = base64.b64encode(jpeg).decode("utf-8")
        try:
            messages = build_openai_messages(b64)
            # Llamada a OpenAI en executor para no bloquear el loop
            response = await loop.run_in_executor(
                None,
                lambda: client.chat.completions.create(
                    user=session_id,
                    model=MODEL,
                    messages=messages,
                    temperature=0,
                    max_tokens=500
                )
            )
            content = response.choices[0].message.content
            logger.debug(content)
            result: dict[str, bool] = json.loads(content)
            r.hset(f"analysis:{session_id}", mapping=result)
            logger.debug(f"‚úîÔ∏è Analysis saved for session {session_id}")
            
            
            
            if result:
               # Encontrar la clave cuyo valor sea m√°ximo
               top_label, top_value = max(result.items(), key=lambda kv: kv[1])
               # 3) Abrir sesi√≥n de DB para hacer upsert en PosturaCount
               db = SessionLocal()
               try:
                   # Intentar obtener la fila existente
                   fila = (
                       db.query(PosturaCount)
                       .filter(
                           PosturaCount.session_id == session_id,
                           PosturaCount.posture_label == top_label
                       )
                       .first()
                   )
                   if fila:
                       # Si existe, incrementamos el contador
                       fila.count += 1
                       db.add(fila)
                   else:
                       # Si no existe, creamos una nueva fila
                       nueva = PosturaCount(
                           session_id=session_id,
                           posture_label=top_label,
                           count=1
                       )
                       db.add(nueva)
                   db.commit()
                   logger.debug(f"‚úîÔ∏è PostureCount updated: {session_id} - {top_label}")
                   # Guardar evento de timeline
                   try:
                       evt = {
                           "timestamp": datetime.utcnow().isoformat(),
                           "postura": top_label,
                           "tiempo_mala_postura": bad_time
                       }
                       r.rpush(f"timeline:{session_id}", json.dumps(evt))
                       r.ltrim(f"timeline:{session_id}", -200, -1)
                   except Exception:
                       logger.exception("Error guardando timeline")
               except Exception:
                   logger.exception("Error actualizando PosturaCount en DB")
                   db.rollback()
               finally:
                   db.close()
            
            
        except Exception:
            logger.exception("Error en an√°lisis OpenAI")
        finally:
            api_analysis_queue.task_done()
            

# En startup, lanza el worker en background
@asynccontextmanager
async def lifespan(app: FastAPI):
    asyncio.create_task(api_analysis_worker())
    logger.debug("‚úÖ API analysis worker scheduled")
    yield  
    
app = FastAPI(lifespan=lifespan)
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

app.include_router(sesiones.router)
app.include_router(pacientes.router)
app.include_router(metricas.router)
app.include_router(analysis.router)
app.include_router(postura_counts.router)
app.include_router(timeline.router)
app.include_router(calibracion.router)
processed_frames_queue = asyncio.Queue(maxsize=10)

@app.websocket("/video/input/{device_id}")
async def video_input(websocket: WebSocket, device_id: str):
    await websocket.accept()
    loop = asyncio.get_running_loop()
    
    # Detectar modo calibraci√≥n: query ?calibracion=1
    calibrating_query = websocket.scope.get("query_string", b"").decode().find("calibracion=1") >= 0
    if calibrating_query:
        await websocket.send_text(json.dumps({"type": "modo", "calibracion": True}))
    else:
        await websocket.send_text(json.dumps({"type": "modo", "calibracion": False}))

    # Variables para manejar PostureMonitor din√°micamente
    posture_monitor = None
    current_session_id = None
    
    try:
        while True:
            # 1. Verificar el session_id desde Redis en cada iteraci√≥n
            redis_shpd_key = f"shpd-data:{device_id}"
            session_id = r.hget(redis_shpd_key, "session_id")
            
            # Si estamos calibrando y a√∫n no hay session_id, crea uno temporal antes de crear PostureMonitor
            # if calibrating_query and not session_id:
            #     session_id = f"calib-{device_id}"
            #     r.hset(redis_shpd_key, mapping={"session_id": session_id})

            # calibrating = calibrating_query or (session_id and str(session_id).startswith("calib-"))
            
            # 2. Si el session_id cambi√≥, reinicializar PostureMonitor
            if session_id != current_session_id:
                logger.info(f"üìã Session ID cambi√≥ de {current_session_id} a {session_id}")
                posture_monitor = PostureMonitor(session_id, save_metrics=not calibrating_query)
                current_session_id = session_id
                logger.info(f"‚úÖ PostureMonitor reinicializado para session {session_id}")

            # 3. Procesar frame normalmente
            data = await websocket.receive_bytes()
            frame = await loop.run_in_executor(None, _decode_jpeg, data)
            if frame is None:
                continue

            # Solo procesar si tenemos un PostureMonitor v√°lido
            if posture_monitor is None:
                # Si no hay PostureMonitor, enviar frame sin procesar
                jpeg = await loop.run_in_executor(None, _encode_jpeg, frame)
            else:
                processed = await loop.run_in_executor(None, posture_monitor.process_frame, frame)
                jpeg = await loop.run_in_executor(None, _encode_jpeg, processed)
            
            if processed_frames_queue.full():
                processed_frames_queue.get_nowait()
            await processed_frames_queue.put(jpeg)

            # 4. Dispara an√°lisis OpenAI si guardaron un frame crudo (solo si hay session_id v√°lido)
            if posture_monitor is not None and not calibrating_query:
                raw_key = f"raw_frame:{session_id}"
                flag_value = r.hget(raw_key, "flag_alert")  
                bad_time = r.hget(raw_key, "bad_time")  
                if flag_value == "1":
                    await api_analysis_queue.put({
                        "session_id": session_id,
                        "jpeg": jpeg,
                        "bad_time": bad_time
                    })
                    r.delete(raw_key)
                    logger.debug(f"‚úîÔ∏è Disparo para analisis ejecutado para sesi√≥n {session_id}")
                
    except WebSocketDisconnect:
        logger.info(f"WebSocket desconectado para device_id: {device_id}")
    except Exception as e:
        logger.error(f"Error en video_input: {e}")
        logger.exception("Detalles del error:")
    finally:
        logger.info(f"Cerrando WebSocket para device_id: {device_id}")

@app.websocket("/video/output")
async def video_output(websocket: WebSocket):
    await websocket.accept()
    try:
        while True:
            jpeg = await processed_frames_queue.get()
            await websocket.send_bytes(jpeg)
    except WebSocketDisconnect:
        pass
    except Exception:
        pass

def _decode_jpeg(data: bytes):
    arr = np.frombuffer(data, np.uint8)
    return cv2.imdecode(arr, cv2.IMREAD_COLOR)

def _encode_jpeg(frame):
    _, buf = cv2.imencode('.jpg', frame, [cv2.IMWRITE_JPEG_QUALITY, 50])
    return buf.tobytes()

Base.metadata.create_all(bind=engine)

if __name__ == "__main__":
    uvicorn.run(
        app,
        host="0.0.0.0",
        port=8765,
        log_level="warning",
        access_log=True
    )
